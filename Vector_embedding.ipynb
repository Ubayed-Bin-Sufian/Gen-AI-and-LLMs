{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ubayed-Bin-Sufian/Gen-AI-and-LLMs/blob/main/Vector_embedding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VBZDi1dwyJtM",
        "outputId": "e97963c0-170f-4715-e8ff-cfe42bb6d436"
      },
      "outputs": [],
      "source": [
        "pip install gensim"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lkv2bnc2Mh1y"
      },
      "source": [
        "# Import trained model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<b>Word2vec</b> is a technique in natural language processing (NLP) for obtaining vector representations of words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qCPkvuHFy16u",
        "outputId": "97c5e33a-e185-43aa-ce2d-ae6f5bf51c55"
      },
      "outputs": [],
      "source": [
        "import gensim.downloader as api\n",
        "model = api.load(\"word2vec-google-news-300\")  # download the model and return as object ready for use\n",
        "# The dimension of the vector using which these words are represented is 300."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ox1KWfS1Msne"
      },
      "source": [
        "# Example of a word as a vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U_AMdizk0wVA",
        "outputId": "aaa9cabf-f1ba-4d85-c323-14d40caeaf6e"
      },
      "outputs": [],
      "source": [
        "word_vectors=model\n",
        "\n",
        "# Let us look how the vector embedding of a word looks like\n",
        "print(word_vectors['computer'])  # Example: Accessing the vector for the word 'computer'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VP4wMXZQsXyf",
        "outputId": "067af890-5063-4d39-d02e-2bc6a106ab5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(300,)\n"
          ]
        }
      ],
      "source": [
        "# 300 numbers are used to represent each words; 300 dimensions\n",
        "print(word_vectors['cat'].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Word Embeddings Visualization**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Get the word vectors for the target words and most similar words \n",
        "target_words = [\"Paris\", \"Italy\", \"France\", \"doctor\", \"law\", \"medicine\"]\n",
        "similar_words = [word for word, _ in model.most_similar(positive=[\"Paris\", \"Italy\"], negative=[\"France\"], topn = 10)] + \\\n",
        "                [word for word, _ in model.most_similar(positive=[\"doctor\", \"law\"], negative=[\"medicine\"], topn = 10)]\n",
        "words = target_words + similar_words\n",
        "word_vectors = [model[word] for word in words]\n",
        "\n",
        "# Convert word_vectors to a NumPy array \n",
        "word_vectors_array = np.array(word_vectors)\n",
        "\n",
        "# Perform t-SNE dimensionality reduction \n",
        "tsne = TSNE(n_components=2, random_state=42, perplexity=5)\n",
        "reduced_vectors = tsne.fit_transform(word_vectors_array)\n",
        "\n",
        "# Plot the word embeddings \n",
        "plt.figure(figsize=(12,10))\n",
        "for i, word in enumerate(words):\n",
        "    plt.scatter(reduced_vectors[i,0], reduced_vectors[i,1], marker=\"o\" if word in target_words else \"x\")\n",
        "    plt.annotate(word, xy=(reduced_vectors[i, 0], reduced_vectors[i, 1]))\n",
        "\n",
        "plt.title(\"Word Embeddings Visualization\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6BqaeOZM0WB"
      },
      "source": [
        "# Similar words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KUPKTu3MOg8j"
      },
      "source": [
        "# King + Woman - Man = ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GtXyb8ERMyZd",
        "outputId": "4106c848-37b5-4e28-f7c5-1b7fd3b18fe7"
      },
      "outputs": [],
      "source": [
        "# Example of using most_similar\n",
        "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=1)) # prints top 1 similarity\n",
        "print(word_vectors.most_similar(positive=['king', 'woman'], negative=['man'], topn=10)) # prints top 10 similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzQ2Ibo3M5DY"
      },
      "source": [
        "# Let us check the similarity b/w a few pair of words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eMx9DkoM802",
        "outputId": "c0a94216-f07c-462d-9aac-db7e96492198"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.76640123\n",
            "0.6510957\n",
            "0.7643474\n",
            "0.8543272\n",
            "0.7594367\n",
            "0.11408084\n"
          ]
        }
      ],
      "source": [
        "# Example of calculating similarity\n",
        "print(word_vectors.similarity('woman', 'man'))\n",
        "print(word_vectors.similarity('king', 'queen'))\n",
        "print(word_vectors.similarity('uncle', 'aunt'))\n",
        "print(word_vectors.similarity('boy', 'girl'))\n",
        "print(word_vectors.similarity('nephew', 'niece'))\n",
        "print(word_vectors.similarity('paper', 'water'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Word Pair Similarities"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The code calculates similarity scores for various word pairs using a language model, storing these pairs and their scores in a list. It then creates a bar plot to visualize these similarity scores, labeling each bar with the corresponding word pair and its similarity score. The plot provides a clear comparison of the semantic similarities between the chosen word pairs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Calculate similarities\n",
        "similarities = [\n",
        "    (\"woman\", \"man\", model.similarity(\"woman\",\"man\")),\n",
        "    (\"king\", \"queen\", model.similarity(\"woman\",\"man\")),\n",
        "    (\"uncle\", \"aunt\", model.similarity(\"uncle\",\"aunt\")),\n",
        "    (\"boy\", \"girl\", model.similarity(\"boy\",\"girl\")),\n",
        "    (\"nephew\", \"niece\", model.similarity(\"nephew\",\"niece\")),\n",
        "    (\"paper\", \"water\", model.similarity(\"paper\",\"water\")),     \n",
        "]\n",
        "\n",
        "# Extract word pairs and similarity scores\n",
        "word_pairs = [pair[:2] for pair in similarities]\n",
        "similarity_scores = [pair[2] for pair in similarities]\n",
        "\n",
        "# Create a bar plot\n",
        "fig, ax = plt.subplots(figsize=(10,6))\n",
        "ax.bar(range(len(word_pairs)), similarity_scores)\n",
        "ax.set_xticks(range(len(word_pairs)))\n",
        "ax.set_xticklabels(word_pairs, rotation=45, ha=\"right\")\n",
        "ax.set_ylabel(\"Similarity Score\")\n",
        "ax.set_title(\"Word Pair Similarities\")\n",
        "\n",
        "# Add similarity scores as text labels above the bars\n",
        "for i, score in enumerate(similarity_scores):\n",
        "    ax.text(i, score + 0.01, f'{score:.2f}', ha=\"center\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clOg4fnsOIqE"
      },
      "source": [
        "# Most similar words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "raqRgaROOKlb",
        "outputId": "9adf1a3a-ceea-4ebc-9680-fcf99a00a2b2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('towers', 0.8531750440597534), ('skyscraper', 0.6417425870895386), ('Tower', 0.639177143573761), ('spire', 0.594687819480896), ('responded_Understood_Atlasjet', 0.5931612253189087)]\n"
          ]
        }
      ],
      "source": [
        "print(word_vectors.most_similar(\"tower\", topn=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X3BXCeFkMxuU"
      },
      "source": [
        "# Now let us see the vector similarity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eo0Q3I5H2naW",
        "outputId": "cdec7362-3135-4395-9da0-e0f4e9a9545d"
      },
      "outputs": [],
      "source": [
        "# # Words to compare\n",
        "# word1 = 'man'\n",
        "# word2 = 'woman'\n",
        "\n",
        "# word3 = 'semiconductor'\n",
        "# word4 = 'earthworm'\n",
        "\n",
        "# word5 = 'nephew'\n",
        "# word6 = 'niece'\n",
        "\n",
        "# word7 = 'male'\n",
        "# word8 = 'female'\n",
        "\n",
        "# word9 = 'phone'\n",
        "# word10 = 'tape'\n",
        "\n",
        "# word11 = 'boy'\n",
        "# word12 = 'girl'\n",
        "\n",
        "# word13 = 'computer'\n",
        "# word14 = 'potato'\n",
        "\n",
        "# word15 = 'uncle'\n",
        "# word16 = 'aunt'\n",
        "\n",
        "# word17 = 'king'\n",
        "# word18 = 'queen'\n",
        "\n",
        "# word19 = 'bottle'\n",
        "# word20 = 'earphone'\n",
        "\n",
        "\n",
        "# # Calculate the vector difference\n",
        "# vector_difference1 = model[word1] - model[word2]\n",
        "# vector_difference2 = model[word3] - model[word4]\n",
        "# vector_difference3 = model[word5] - model[word6]\n",
        "# vector_difference4 = model[word7] - model[word8]\n",
        "# vector_difference5 = model[word9] - model[word6]\n",
        "# vector_difference6 = model[word11] - model[word12]\n",
        "# vector_difference7 = model[word13] - model[word14]\n",
        "# vector_difference8 = model[word15] - model[word16]\n",
        "# vector_difference9 = model[word17] - model[word18]\n",
        "# vector_difference10 = model[word19] - model[word20]\n",
        "\n",
        "\n",
        "# # Calculate the magnitude of the vector difference\n",
        "# magnitude_of_difference1 = np.linalg.norm(vector_difference1)\n",
        "# magnitude_of_difference2 = np.linalg.norm(vector_difference2)\n",
        "# magnitude_of_difference3 = np.linalg.norm(vector_difference3)\n",
        "# magnitude_of_difference4 = np.linalg.norm(vector_difference4)\n",
        "# magnitude_of_difference5 = np.linalg.norm(vector_difference5)\n",
        "# magnitude_of_difference6 = np.linalg.norm(vector_difference6)\n",
        "# magnitude_of_difference7 = np.linalg.norm(vector_difference7)\n",
        "# magnitude_of_difference8 = np.linalg.norm(vector_difference8)\n",
        "# magnitude_of_difference9 = np.linalg.norm(vector_difference9)\n",
        "# magnitude_of_difference10 = np.linalg.norm(vector_difference10)\n",
        "\n",
        "\n",
        "# # Print the magnitude of the difference\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word1, word2, magnitude_of_difference1))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word3, word4, magnitude_of_difference2))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word5, word6, magnitude_of_difference3))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word7, word8, magnitude_of_difference4))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word9, word10, magnitude_of_difference5))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word11, word12, magnitude_of_difference6))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word13, word14, magnitude_of_difference7))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word15, word16, magnitude_of_difference8))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word17, word18, magnitude_of_difference9))\n",
        "# print(\"The magnitude of the difference between '{}' and '{}' is {:.2f}\".format(word19, word20, magnitude_of_difference10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The above code could be optimised to the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Vector Differences in 2D Space**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Words to compare\n",
        "word_pairs = [\n",
        "    (\"boy\", \"girl\"),\n",
        "    (\"semiconductor\", \"earthworm\"),\n",
        "    (\"computer\", \"potato\"),\n",
        "    (\"king\", \"queen\"),\n",
        "    (\"bottle\", \"earphone\"),    \n",
        "    ]\n",
        "\n",
        "# Calculate the vector differences and magnitudes\n",
        "vector_differences = []\n",
        "magnitudes = []\n",
        "\n",
        "for word1, word2 in word_pairs:\n",
        "    vector_difference = model[word1] - model[word2]\n",
        "    magnitude = np.linalg.norm(vector_difference)\n",
        "    vector_differences.append(vector_difference)\n",
        "    magnitudes.append(magnitude)\n",
        "\n",
        "# Print the magnitude of the differences \n",
        "for i, (word1, word2) in enumerate(word_pairs):\n",
        "    print(f\"The magnitude of the difference between '{word1}' and '{word2}' is {magnitudes[i]:.2f}\")\n",
        "\n",
        "# Create a bar plot for the magnitudes \n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "ax.bar(range(len(word_pairs)), magnitudes)\n",
        "ax.set_xticks(range(len(word_pairs)))\n",
        "ax.set_xticklabels([f\"{word1}-{word2}\" for word1, word2 in word_pairs], rotation=45, ha=\"right\")\n",
        "ax.set_ylabel(\"Magnitude of Vector Differences\")\n",
        "ax.set_title(\"Magnitude of Vector Differences\")\n",
        "\n",
        "# Add magnitude values as text labels above the bars\n",
        "for i, magnitude in enumerate(magnitudes):\n",
        "    ax.text(i, magnitude + 0.1, f'{magnitude:.2f}', ha=\"center\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Perform PCA on the vector differences\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_vectors_differences = pca.fit_transform(vector_differences)\n",
        "\n",
        "\n",
        "# Create a scatter plot for the vector differences\n",
        "fig, ax = plt.subplots(figsize=(8, 6))\n",
        "ax.scatter(reduced_vectors_differences[:, 0], reduced_vectors_differences[:, 1])\n",
        "\n",
        "for i, (word1, word2) in enumerate(word_pairs):\n",
        "    ax.annotate(f\"{word1}-{word2}\", (reduced_vectors_differences[i, 0], reduced_vectors_differences[i, 1]))\n",
        "\n",
        "ax.set_xlabel(\"Principal Component 1\")\n",
        "ax.set_ylabel(\"Principal Component 2\")\n",
        "ax.set_title(\"Vector Differences in 2D Space\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
