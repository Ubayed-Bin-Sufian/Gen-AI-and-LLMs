{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of **model_id** variable is to identify the specific pre-trained model used for fine-tuning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the foundation model to fine-tune\n",
    "model_id = \"huggingface-llm-falcon-7b-bf16\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Amazon SEC filing training data stored in an S3 bucket.\n",
    "from sagemaker.jumpstart.utils import get_jumpstart_content_bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the bucket containing sample training data\n",
    "data_bucket = get_jumpstart_content_bucket(aws_region)\n",
    "data_prefix = \"training-datasets/sec_data\"\n",
    "\n",
    "# Define S3 paths for training and validation datasets\n",
    "training_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/train/\"\n",
    "validation_dataset_s3_path = f\"s3://{data_bucket}/{data_prefix}/validation/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare training hyperparameters\n",
    "from sagemaker import hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The role of **my_hyperparameters** is to set the training options like epochs and batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve default hyperparameters for the selected model\n",
    "my_hyperparameters = hyperparameters.retrieve_default(\n",
    "    model_id=model_id, model_version=model_version\n",
    ")\n",
    "\n",
    "# Customize hyperparameters\n",
    "my_hyperparameters[\"epoch\"] = \"3\"\n",
    "my_hyperparameters[\"per_device_train_batch_size\"] = \"2\"\n",
    "my_hyperparameters[\"instruction_tuned\"] = \"False\"\n",
    "print(my_hyperparameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate hyperparameters\n",
    "hyperparameters.validate(\n",
    "    model_id=model_id, model_version=model_version, hyperparameters=my_hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the JumpStart estimator for domain adaptation\n",
    "from sagemaker.jumpstart.estimator import JumpStartEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_adaptation_estimator = JumpStartEstimator(\n",
    "    model_id=model_id,\n",
    "    hyperparameters=my_hyperparameters,\n",
    "    instance_type=\"ml.p3dn.24xlarge\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **fit() method** fine-tunes the model on the specified datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training, specifying paths for training and validation data\n",
    "domain_adaptation_estimator.fit(\n",
    "    {\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training performance metrics like loss and accuracy\n",
    "from sagemaker import TrainingJobAnalytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch metrics from the latest training job\n",
    "training_job_name = domain_adaptation_estimator.latest_training_job.job_name\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy inference endpoints for both fine-tuned and pre-trained models for comparison\n",
    "domain_adaptation_predictor = domain_adaptation_estimator.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deploy the pre-trained model for comparison\n",
    "my_model = JumpStartModel(model_id=model_id)\n",
    "pretrained_predictor = my_model.deploy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters for generating inference responses\n",
    "parameters = {\n",
    "    \"max_new_tokens\": 300,\n",
    "    \"top_k\": 50,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary purpose of the **generate_response** function in the code is to generate text based on a given prompt using the trained model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to query an endpoint and generate response text\n",
    "def generate_response(endpoint_name, text):\n",
    "    payload = {\"inputs\": f\"{text}:\", \"parameters\": parameters}\n",
    "    query_response = query_endpoint_with_json_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    generated_texts = parse_response(query_response)\n",
    "    print(f\"Response: {generated_texts}{newline}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test phrases specific to the SEC domain\n",
    "test_paragraph_domain_adaption = [\n",
    "    \"This Form 10-K report shows that\",\n",
    "    \"We serve consumers through\",\n",
    "    \"Our vision is\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare responses from pre-trained and fine-tuned models\n",
    "for paragraph in test_paragraph_domain_adaption:\n",
    "    print(\"-\" * 80)\n",
    "    print(paragraph)\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{bold}pre-trained{unbold}\")\n",
    "    generate_response(pretrained_predictor.endpoint_name, paragraph)\n",
    "    print(f\"{bold}fine-tuned{unbold}\")\n",
    "    generate_response(domain_adaptation_predictor.endpoint_name, paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up by deleting SageMaker endpoints\n",
    "pretrained_predictor.delete_model()\n",
    "pretrained_predictor.delete_endpoint()\n",
    "domain_adaptation_predictor.delete_model()\n",
    "domain_adaptation_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
